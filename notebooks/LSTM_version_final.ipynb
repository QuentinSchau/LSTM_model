{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnH85aHpd79c"
   },
   "source": [
    "# Prédiction de séries temporelle avec Python\n",
    "\n",
    "Dans ce notebook, nous allons essayer de préduire le cours du bitcoin minute par minute en utilisant un réseau de neurone. Le réseau choisi est un LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZdrhTHId79i"
   },
   "source": [
    "## Import Général"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1659718539738,
     "user": {
      "displayName": "Quentin Schau",
      "userId": "06475432810194919567"
     },
     "user_tz": -120
    },
    "id": "oMgtXzU5d79j"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from skorch import NeuralNetRegressor\n",
    "import pandas as pd\n",
    "from skorch.callbacks import LRScheduler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# pour installer les packages \n",
    "# conda install pytorch==1.9.0 cudatoolkit=11.3 -c pytorch\n",
    "# pip install torch==1.9.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "# conda install -c anaconda scikit-learn\n",
    "# pip install -U scikit-learn\n",
    "\n",
    "# conda install -c conda-forge skorch\n",
    "# pip install -U skorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfvSmw9md79m"
   },
   "source": [
    "# Les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyCVfY4Kd79n"
   },
   "source": [
    "## Etape 0 : la collecte des données\n",
    "\n",
    "Comme nous partons de rien, nous devons d'abord collecter les données. Pour cela, il existe plusieurs solutions. Avant de pouvoir collecter les données, il faut savoir où les trouver.\n",
    "\n",
    "En faisant une recherche sur google, on trouve beaucoup de site web proposant le cours du bitcoin. Pour ne citer que les plus connues : https://fr.finance.yahoo.com/ ou https://www.google.com/finance/?hl=fr\n",
    "\n",
    "Une fois qu'on sait où trouver les données, il faut l'extraire. Pour cela, nous pouvons utiliser le web scrapping. Il s'agit de la première approche que j'ai choisi d'utiliser. Elle comporte son lot d'avantage et d'inconvénient. Vous retrouverez les tableaux récapitulatifs ci-dessous. \n",
    "\n",
    "### Selenium \n",
    "    \n",
    "&nbsp; &nbsp; &nbsp; Selenium est une librairie développée sur plusieurs langage et disponible sur python. Cette librairie permet de réaliser toutes les interactions faisables sur un navigateur web. Une fois qu'on a bien configuré Selenium et qu'on a pris ses marques avec il est relativement facile à utiliser. On peut facilement adapter son script à un autre site web. Il suffit juste de connaître les balises html adéquat. Voici la documentation de la librairie : https://selenium-python.readthedocs.io/. \n",
    "    \n",
    "&nbsp; &nbsp; &nbsp; Ainsi, le script mis en place actualisait toutes les minutes la page web et récupérer la valeur du cours affichée sur le site. Puis on enregistrait ces valeurs dans un fichier csv.\n",
    "\n",
    "&nbsp; &nbsp; &nbsp; Le problème de cette solution est qu'elle nécessite une bonne bande passante. Car à chaque minute, nous actualisons la page du site. Si cette page est volumineuse et si votre réseau internet a de la latence alors vous n'aurez pas le cours minutes par minutes. C'est ce qu'il s'est passé lors de la mise en place de cette solution. Il me manquait des valeurs pour certaines minutes. Ce qui peut fortement impacter notre modèle, car il nous faut des données continues dans le temps. Une solution pour remédier à ce problème aurait été de remplacer les valeurs manquantes par des valeurs fictives qu'on doit créer. On peut faire se procéder avec des interpolations ou en faisant la moyenne des valeurs qui précédent et suivent notre valeur manquante. C'est pourquoi, l'autre solution a été privilégiée.\n",
    "\n",
    "### API REST\n",
    "\n",
    "&nbsp; &nbsp; &nbsp; Une autre solution possible pour récupérer les données consiste à utiliser les requêtes API REST envoyées lors de la navigation sur le site. Cette méthode a l'avantage d'être plus fiable, car nous ne rechangeons pas toute la page, mais nous demandons seulement les données qui nous intéressent. Ainsi une fois que nous avons identifié la bonne requête à envoyer (ceci peut se faire via l'onglet network de la touche F12), nous pouvons facilement enregistrer les données via un script et automatiser ce script sur un serveur via CRON par exemple. C'est ce qui est mis en place actuellement.\n",
    "\n",
    "&nbsp; &nbsp; &nbsp; Ainsi, avec cette solution, la requête API nous permet de récupérer la valeur du bitcoin minute par minute et ceux sur les 5 derniers jours. Donc, nous lançons le script tous les 5 jours puis nous fusionnons les données avec celle déjà enregistrées. L'avantage, c'est que nos données sont continues, car c'est le site web qui nous les envoie. \n",
    "\n",
    "&nbsp; &nbsp; &nbsp; Le problème avec cette solution, est qu'il est assez difficile d'identifier la bonne requête API sans aucune documentation du site. Il faut alors décortiquer les requêtes du site et trouver les bons paramètres, etc. De plus, cette méthode à l'inconvénient de ne pas s'adapter à d'autres sites. Si nous voulons récolter des données sur un autre site alors il faut tout réanalyser les requêtes de ce dernier. \n",
    "\n",
    "<table style=\"border: 1px solid #333;table-layout:fixed;\">\n",
    "    <thead style=\"background-color: #333;color: #fff;\">\n",
    "        <tr>\n",
    "            <th colspan=\"2\" style=\"text-align:center;\">Selenium</th>\n",
    "            <th colspan=\"2\" style=\"text-align:center;\">API REST</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"width: 25%;\"> ➕ Avantages</td>\n",
    "            <td style=\"width: 25%;\"> ➖ Inconvénients </td>\n",
    "            <td style=\"width: 25%;\"> ➕ Avantages</td>\n",
    "            <td style=\"width: 25%;\"> ➖ Inconvénients </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"width: 25%;\" >\n",
    "              <ol>\n",
    "                <li>Facile à déployer sur plusieurs sites </li>\n",
    "                <li>Faire plusieurs actions de manières simple </li>\n",
    "              </ol>\n",
    "            </td>\n",
    "            <td style=\"width: 25%;\"> \n",
    "              <ol>\n",
    "                <li> Configuration peut être un peu laborieuse avec les pilotes à installer </li>\n",
    "                <li> Nécéssite une bonne bande passante pour recharger une page web régulièrement </li>\n",
    "              </ol> \n",
    "            </td>\n",
    "            <td style=\"width: 25%;\"> \n",
    "              <ol>\n",
    "                <li> Très fiable, permet de récupérer des données sans trop de discontinuité</li>\n",
    "                <li> facile à automatiser avec une tâche CRON </li>\n",
    "              </ol>\n",
    "            </td>\n",
    "            <td style=\"width: 25%;\"> \n",
    "              <ol>\n",
    "                <li> Ne fonctionne plus si votre site change ses requêtes</li>\n",
    "                <li> Analyser le traffic réseau lors de la navigation sur le site pour trouver la bonne REST </li>\n",
    "              </ol>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "Les données sont les valeurs du bitcoin avec différentes informations relatives au cours. Comme la valeur à la clôture ou à la fermeture par exemple. Nous devons traiter les données pour pouvoir les exploiter.\n",
    "\n",
    "Pour cela, il existe une bibliothèque sur python : *pandas*. Pandas nous permet de générer des données et de réaliser du traitement dessus assez facilement.\n",
    "\n",
    "Vous retrouverez la documentation de Pandas sur le lien suivant : https://pandas.pydata.org/docs/.\n",
    "\n",
    "## Etape 1 : Charger les données\n",
    "\n",
    "Nous chargeons les données stockées dans un fichier csv dans un objet DataFrame de *Pandas*. On précise que nos données sont séparées par un ';' et que la colonne correspondant à l'index est la colonne 'time'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3493,
     "status": "ok",
     "timestamp": 1659724403907,
     "user": {
      "displayName": "Quentin Schau",
      "userId": "06475432810194919567"
     },
     "user_tz": -120
    },
    "id": "LwE19y1Sd79t",
    "outputId": "9e915ded-d2f1-49ac-ac40-dbcf7c6726ed"
   },
   "outputs": [],
   "source": [
    "dataFromExcel = pd.read_csv('bitcoin.csv',sep=';',index_col='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRSjDKTid79v"
   },
   "source": [
    "## Etape 2 : nettoyer les données\n",
    "\n",
    "&nbsp; &nbsp; &nbsp; Cette étape est très importante. Elle permet de traiter les données manquantes ou les données erronées. Cela peut être des données avec des informations manquantes par exemple. Dans notre cas, nous n'avons aucune donnée manquante. Toutes les données que nous avons sont continues dans le temps.\n",
    "\n",
    "## Etape 3 : traiter les données\n",
    "\n",
    "&nbsp; &nbsp; &nbsp; Maintenant, nous devons traiter les données. C'est-à-dire que nous avons des données qui sont différentes entre elles. Par exemple, nous pouvons imaginer que nous avons des données avec comme première caractéristique un poids en Tonne et nous pouvons imaginer une autre caractéristique avec un poids en g. L'écart d'échelle est très grand et la comparaison des deux caractéristiques n'est pas possible. L'étude des corrélations devient alors plus compliquée à réaliser avec des plages de variation très grande.\n",
    "\n",
    "&nbsp; &nbsp; &nbsp; Ainsi, nous normalisons les données pour pouvoirs mieux les utiliser. Normaliser les données veut dire que la plage de variation de celle-ci se situe entre 0 et 1. L'autre avantage de normaliser les données, c'est que cela demande moins de ressource aux algorithmes pour faire les calcules.\n",
    "\n",
    "&nbsp; &nbsp; &nbsp; Ici, nous utilisons la normalisation standard qui consiste à soustraire la moyenne et à la diviser par l’écart-type soit X l'ensemble de nos données, on normalise X par $|X| = \\frac{X - \\mu}{\\sigma}$. Ou μ représente la moyenne et σ représente l'écart-type. Dans ce cas, chaque valeur refléterait la distance par rapport à la moyenne en unités d’écart-type. Si nous supposons que toutes les variables proviennent d’une distribution normale, la normalisation les rapprocherait toutes de la distribution normale standard. La distribution résultante a une moyenne de 0 et un écart-type de 1.\n",
    "\n",
    "&nbsp; &nbsp; &nbsp; D'autres normes existent comme la normalisation de moyenne (normalisation min max), dont la distribution résultante sera entre -1 et 1 avec une moyenne = 0. Ou les normes $L_p$ et encore bien d'autres, cette notion est découle de l'algèbre cf https://fr.wikipedia.org/wiki/Norme_(math%C3%A9matiques)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1659718597358,
     "user": {
      "displayName": "Quentin Schau",
      "userId": "06475432810194919567"
     },
     "user_tz": -120
    },
    "id": "fRHdypv5d79w"
   },
   "outputs": [],
   "source": [
    "# Fonction qui traite le DataFrame, il est subit la normalisation standard\n",
    "# Input : \n",
    "#     - un DataFrame qu'on souhaite normaliser\n",
    "# Output : \n",
    "#     - le DataFrame normaliser\n",
    "#     - la moyenne (ou les moyennes) du DataFrame\n",
    "#     - l'écart-type (ou les écarts-types) du DataFrame\n",
    "def prepross_data(data):\n",
    "    #enlève les valeurs vides\n",
    "    data.dropna(axis=0,inplace=True)\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    data =(data-mean)/std\n",
    "    return mean,std,data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAhLNVIhd79y"
   },
   "source": [
    "Maintenant que nous avons normé nos données, il faut formaliser une structure de donnée pour pouvoir les utiliser avec nos algorithmes. Il faut pour cela choisir un nombre de caractéristiques qu'on peut extraire de nos données. Cela peut-être les valeurs brutes ou on peut réaliser préalablement un traitement sur les données tel qu'une combinaison linéaire des données brutes.\n",
    "\n",
    "Ainsi construisons un vecteur $\\overrightarrow{x}$ qui sera passé à notre modèle pour l'apprentissage mais aussi pour la prédiction. \n",
    "\n",
    "Ce vecteur est composé de caractéristiques (features). Il nous reste à définir ce nombre de caractéristiques notons le $N_{features}$ . \n",
    "\n",
    "Donc $\\overrightarrow{x} \\in \\mathbb{R}^{N_{features}} \\;$ .\n",
    "\n",
    "Ici les features correspondent à la valeur du bitcoin à la clôture à chaque minute. Nous choisissons donc un vecteur qui contient les valeurs du bitcoin à la clôture sur $N_{features}$ minutes consécutives.\n",
    "\n",
    "Une autre possibilité est de passer les valeurs de la clôture, l'ouverture et d'autre valeur qu'on connait ou qu'on a calculé chaque minutes.\n",
    "\n",
    "Au lieu de passer un seul vecteur à notre modèle pour qu'il puisse apprendre. Il existe un mode appelé le mode batch, qui permet de passer plusieurs vecteurs à notre modèle ce qui lui permet d'apprendre sur plusieurs vecteurs. Cela permet d'éviter certains biais introduits par un vecteur car notre modèle moyenne l'apprentissage sur l'ensemble des vecteurs.\n",
    "\n",
    "Maintenant, il faut labelliser les données pour pouvoir indiquer à notre modèle quelle est valeur de terrain par rapport à celle qui a prédit. Concrètement, imaginons que nous ayons les valeurs du bitcoin sur 6 minutes, les 5 premières minutes correspondent aux caractéristiques (features) et forment un « vecteur de features » et la $6^{ème}$ minute correspond au label associé à ce vecteur. Ainsi, il pourra calculer son erreur entre la valeur prédite et la vraie valeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1659718599614,
     "user": {
      "displayName": "Quentin Schau",
      "userId": "06475432810194919567"
     },
     "user_tz": -120
    },
    "id": "SVopGfKJd79x"
   },
   "outputs": [],
   "source": [
    "#Constante\n",
    "\n",
    "numFeature = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1659718600538,
     "user": {
      "displayName": "Quentin Schau",
      "userId": "06475432810194919567"
     },
     "user_tz": -120
    },
    "id": "FTXgVcOfd79x"
   },
   "outputs": [],
   "source": [
    "# Fonction qui créer un vecteur x à partir d'un index et d'un DataFrame\n",
    "# Input : \n",
    "#     - l'index sur la première valeur du vecteur\n",
    "#     - un DataFrame contenant toutes les valeurs\n",
    "# Output : un vecteur x, contenant les valeur de cloture du bitcoin des N prochaines minutes à partir de l'index. \n",
    "def create_input(index,pdData):\n",
    "    return pdData[\"cloture\"].iloc[index:index+numFeature+1].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1659718601455,
     "user": {
      "displayName": "Quentin Schau",
      "userId": "06475432810194919567"
     },
     "user_tz": -120
    },
    "id": "VcIRibDqd79z"
   },
   "outputs": [],
   "source": [
    "# Méthode qui permet de convertir un index de notre DataFrame en un dateTime\n",
    "def convert_index_to_datetime(index):\n",
    "    #la première date de notre base de donnée\n",
    "    date = datetime.datetime.strptime(str(index)[:6],\"%y%m%d\")\n",
    "    hour = int(str(index)[7:-1])//60\n",
    "    minute = int(str(index)[7:-1])%60\n",
    "    index_datetime = date + datetime.timedelta(hours=hour) + datetime.timedelta(minutes=minute)\n",
    "    return index_datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1659718602323,
     "user": {
      "displayName": "Quentin Schau",
      "userId": "06475432810194919567"
     },
     "user_tz": -120
    },
    "id": "7TeU7WABd790"
   },
   "outputs": [],
   "source": [
    "#Methode qui créer un batch contenant l'ensemble des vecteurs et des labels associées utilisé lors de l'apprentissage.\n",
    "# input: DataFrame contenant les données\n",
    "# output: le batch contenant tous les vectueurs du DataFrame avec les labels\n",
    "def create_batchs_and_labels(data):\n",
    "\n",
    "    iterator = 0\n",
    "    labels =[]\n",
    "    batch =  []\n",
    "    #On boucle sur la base de donnée :\n",
    "    while (iterator <= data.shape[0]-numFeature-1):   \n",
    "\n",
    "        dataExtracted = create_input(iterator,data)\n",
    "        if (len(dataExtracted) == numFeature+1):\n",
    "            sequence = []\n",
    "            for x in dataExtracted[:-1]:\n",
    "                sequence.append([x])\n",
    "            batch.append(sequence)\n",
    "            labels.append([dataExtracted[-1]])\n",
    "        iterator+=1\n",
    "    return torch.FloatTensor(batch),torch.FloatTensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-Ioc4qWd791"
   },
   "source": [
    "Pour avoir une structure plus facile d'utilisation, on va utiliser la bibliothèque pytorch. On va utiliser l'object Dataset. Ce qui va nous permettre de mieux gérer nos données pour l'apprentissage. Voici la documentation : https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1659718604631,
     "user": {
      "displayName": "Quentin Schau",
      "userId": "06475432810194919567"
     },
     "user_tz": -120
    },
    "id": "GjTrOZ-EKnx1"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Dataset(Dataset):    \n",
    "    def __init__(self, data_file):\n",
    "        pf = pd.read_csv(str(data_file),sep=';',index_col='time')\n",
    "        self.mean,self.std,self.prepross_pf = prepross_data(pf)\n",
    "        self.batch,self.labels = create_batchs_and_labels(self.prepross_pf)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.batch.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.batch[idx],self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "em5IakC8LR6Q"
   },
   "source": [
    "\n",
    "Ainsi on créer notre Dataset à partir de notre csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1725,
     "status": "ok",
     "timestamp": 1659718608598,
     "user": {
      "displayName": "Quentin Schau",
      "userId": "06475432810194919567"
     },
     "user_tz": -120
    },
    "id": "_JlKSKGNKnx1"
   },
   "outputs": [],
   "source": [
    "# training = Dataset(\"./bitcoin.csv\")\n",
    "training = Dataset(\"bitcoin.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJOREUNYLo1l"
   },
   "source": [
    "Puis on créer un Dataloader qui va charger les données pour pouvoir les utiliser et faire des calculs sur celles-ci. Voir la documentation pour plus d'information https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#preparing-your-data-for-training-with-dataloaders.\n",
    "\n",
    "Ici, on demande de faire une permutation sur nos vecteurs pour que l'apprentissage soit fait sur des vecteurs qui ne sont pas liés les uns aux autres par leur temporalité. Ici, le but de notre modèle est d'avoir en entrée un vecteur contenant les valeurs de clôture des $N_{feature}$ dernières minutes et le modèle doit prédire la valeur de la $N_{feature} + 1$ minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1659718618619,
     "user": {
      "displayName": "Quentin Schau",
      "userId": "06475432810194919567"
     },
     "user_tz": -120
    },
    "id": "GPOT-uhmKnx1"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training, batch_size=5000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0VISfMxMAi-"
   },
   "source": [
    "Voici du code pour afficher la tailler des différents batchs qu'on a ainsi chargé pour l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1659718622446,
     "user": {
      "displayName": "Quentin Schau",
      "userId": "06475432810194919567"
     },
     "user_tz": -120
    },
    "id": "3BCY95GFKnx2",
    "outputId": "a95ba66f-1882-4664-b7e1-c62488c0c30f"
   },
   "outputs": [],
   "source": [
    "for train_features, train_labels in iter(train_dataloader):\n",
    "    print(f\"taille d'un batch de feature : {train_features.size()}\")\n",
    "    print(f\"taille d'un batch de labels : {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation pytorch du LSTM : https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM\n",
    "\n",
    "Ici pour notre modèle nous choisissons une taille de 150 neurones pour la couche caché. Avec une seul couche recurrente (une cellule LSTM). Input_size correspond au nombre de feature qui seront passé dans le vecteur x. Ici nous avons qu'une seul feature à savoir la valeur à la clôture. Ne pas confondre input_size avec sequence length. Le premier correspond au nombre de features (caractéristiques) tandis que le second correspond à la taille du vecteur x qui est passé en entrée. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size=150, output_size=1,num_layers=1):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lstm = nn.LSTM(input_size,hidden_layer_size,num_layers,batch_first = True)\n",
    "        # linear layer pour préduire \n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        self.hidden_cell = (torch.zeros(1,input_batch.size(0),self.hidden_layer_size,device=input_batch.device),\n",
    "                            torch.zeros(1,input_batch.size(0),self.hidden_layer_size,device=input_batch.device))\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_batch, self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out[:,-1,:])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On définit maintenant le modèle skorch pour pouvoir faire notre entrainement. \n",
    "\n",
    "On réalise un gridSearch pour déterminer le meilleur nombre de neurones. On réalise les calcules sur la carte graphique. \n",
    "\n",
    "Voici la documentation de Skorch pour plus d'information : https://skorch.readthedocs.io/en/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_regr = NeuralNetRegressor(\n",
    "    LSTM,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    max_epochs=1000,\n",
    "    lr=0.001,\n",
    "    callbacks=[\n",
    "        LRScheduler(policy='StepLR', step_size=100, gamma=0.1)\n",
    "    ],\n",
    "    criterion=nn.MSELoss,\n",
    "    device='cuda' \n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "#paramètre qu'on va déterminer avec le GridSearch\n",
    "param_grid = {'module__hidden_layer_size': [25,50,75,100,125,150,175,200]}\n",
    "\n",
    "grid = GridSearchCV(net_regr,param_grid,cv=3,error_score='raise')\n",
    "   \n",
    "#on réalise l'entrainement\n",
    "grid.fit(train_features,train_labels)\n",
    "\n",
    "# Sauvegarde du modèle \n",
    "saveModel = datetime.today().strftime('%Y%m%d_%H_%M')\n",
    "# cf doc for more information https://skorch.readthedocs.io/en/stable/user/save_load.html\n",
    "joblib.dump(grid.best_estimator_,'saving_model/model/'+saveModel+'.pkl')\n",
    "\n",
    "# loading need to initialize net\n",
    "# net_regr.initialize()  # This is important!\n",
    "# net_regr.load_params(f_params='some-file.pkl')\n",
    "\n",
    "# on affiche le meilleur modèle et ses paramètres\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "colab": {
   "collapsed_sections": [],
   "name": "Post_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
